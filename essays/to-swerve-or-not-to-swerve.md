---
layout: essay
type: essay
title: Why UI Frameworks? 
# All dates must be YYYY-MM-DD format!
date: 2019-02-21
labels:
  - Software Engineering
  - Ethics
  - Autonomous Cars
---


<div class="ui container">
<h1>Ethics</h1>

<p>
What are ethics? I'm sure many people can give examples off the top of their head but it's a bit harder to narrow
it down to a singular definition. Some might get flustered and say something along the lines of "To be ethical is to 
make ethical decisions!" Giving a definition with the word you're trying to define in it doesn't usually help much so let
us try to define it a bit better.
</p>
<p>
Straight from dictionary ethics are defined as "moral principles that govern a person's behavior or the conducting of 
an activity." Which is a good definition, but it leads to more questions. What are moral principles? Who decides what 
is moral and immoral? I believe morals are simply our own personal definition of right and wrong. Not everything is 
black and white, what might seem immoral to one, could be moral to the next. There are of course big topics such as 
murder that the majority side with immoral. So I would say that ethics as a whole are what the majority of society 
deem moral, or right.
</p>

<h1>Case: Autonomous Cars</h1>

<img class="ui large right floated image" src="/images/fender-bender.jpg">
<p>
From the paper <a href="https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/">
Why Self-Driving Cars Must Be Programmed to Kill</a> a major ethical decision is proposed. To have self driving cars 
one must ask themself, when the car is faced with a moral dillema, what should it do? The situation described in the 
paper is a harrowing one. Say you're driving along the road and a group of ten people are in front of you. You can't 
stop in time, but the car could swerve to the left only killing the driver. Would it be moral to program all cars to 
choose to kill their driver over causing more harm?
</p>
<p> 
I definitely wouldn't want to be the decision maker  behind that, but I will try to dissect the issues. One might 
jump at the logic and say well yes, it should always kill less people. But, food for thought, the driver had no control
over what was happening. The driver had no control over the situation he or she was put in. They simply purchased a 
product and expected it to keep them safe, so why should they pay the price? Outside looking in it seems cut and dry, 
but I know I don't want to die over someone elses mistake. </p>

<p>
So if this situation did play out, who would be to blame? Humans by nature are imperfect. And as much as they try 
to design perfect systems, imperfections will arise. So, who is to blame? Do we blame the car and haul it off to the 
nearest scrapyard to be crushed into a small cube and left to rot as punishment? Do we haul the software engineers 
infront of a jury? The company? Is the company even liable when posed with a unavoidable siutation?
</p>

<h1>Code of Ethics</h1>
<p>
The <a href="https://www.acm.org/code-of-ethics"> ACM code of ethics </a> list one big bullet. To do no harm. 
Looking back at the case study one might say that they clearly violated this, so lets haul off the engineer to the 
headsmen. But what we don't immediately thinka bout is intent. Did the enginneer intend to cause harm? 
</p>
<p>
I'm a believer that without intent, there really is no crime in most cases. If the engineer knew about the issue and 
did nothing to rectify it, that would be a completely different story. 
</p>
</div>
